{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torchvision import *\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "#import transforms\n",
    "from torchvision import transforms\n",
    "import Data.IngredientsLoader as IngredientsLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import copy\n",
    "import os\n",
    "\n",
    "#choose random seed\n",
    "seed = 42\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "#hyperparameters\n",
    "batch_size = 4\n",
    "learning_rate = 1e-3\n",
    "momentum=0.9\n",
    "transforms = transforms.Compose(\n",
    "[\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((224,224))\n",
    "])\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "use_cuda = False"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-2.8.0-py3-none-any.whl (452 kB)\n",
      "     -------------------------------------- 452.9/452.9 kB 1.6 MB/s eta 0:00:00\n",
      "Collecting huggingface-hub<1.0.0,>=0.2.0\n",
      "  Using cached huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n",
      "Collecting responses<0.19\n",
      "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Collecting dill<0.3.7\n",
      "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
      "     -------------------------------------- 110.5/110.5 kB 6.3 MB/s eta 0:00:00\n",
      "Collecting aiohttp\n",
      "  Downloading aiohttp-3.8.3-cp39-cp39-win_amd64.whl (323 kB)\n",
      "     ------------------------------------- 323.5/323.5 kB 10.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\cs_hack\\python3.9\\lib\\site-packages (from datasets) (1.22.3)\n",
      "Collecting multiprocess\n",
      "  Downloading multiprocess-0.70.14-py39-none-any.whl (132 kB)\n",
      "     -------------------------------------- 132.9/132.9 kB 7.7 MB/s eta 0:00:00\n",
      "Collecting fsspec[http]>=2021.11.1\n",
      "  Downloading fsspec-2022.11.0-py3-none-any.whl (139 kB)\n",
      "     -------------------------------------- 139.5/139.5 kB 8.1 MB/s eta 0:00:00\n",
      "Collecting xxhash\n",
      "  Downloading xxhash-3.2.0-cp39-cp39-win_amd64.whl (30 kB)\n",
      "Requirement already satisfied: packaging in d:\\cs_hack\\python3.9\\lib\\site-packages (from datasets) (21.3)\n",
      "Collecting pyyaml>=5.1\n",
      "  Using cached PyYAML-6.0-cp39-cp39-win_amd64.whl (151 kB)\n",
      "Requirement already satisfied: pandas in d:\\cs_hack\\python3.9\\lib\\site-packages (from datasets) (1.4.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in d:\\cs_hack\\python3.9\\lib\\site-packages (from datasets) (2.28.1)\n",
      "Collecting pyarrow>=6.0.0\n",
      "  Downloading pyarrow-10.0.1-cp39-cp39-win_amd64.whl (20.3 MB)\n",
      "     ---------------------------------------- 20.3/20.3 MB 8.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: tqdm>=4.62.1 in d:\\cs_hack\\python3.9\\lib\\site-packages (from datasets) (4.64.0)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-6.0.4-cp39-cp39-win_amd64.whl (28 kB)\n",
      "Collecting yarl<2.0,>=1.0\n",
      "  Downloading yarl-1.8.2-cp39-cp39-win_amd64.whl (56 kB)\n",
      "     ---------------------------------------- 56.8/56.8 kB 3.1 MB/s eta 0:00:00\n",
      "Collecting async-timeout<5.0,>=4.0.0a3\n",
      "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "Collecting aiosignal>=1.1.2\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in d:\\cs_hack\\python3.9\\lib\\site-packages (from aiohttp->datasets) (2.1.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in d:\\cs_hack\\python3.9\\lib\\site-packages (from aiohttp->datasets) (21.4.0)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.3.3-cp39-cp39-win_amd64.whl (34 kB)\n",
      "Requirement already satisfied: filelock in d:\\cs_hack\\python3.9\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\cs_hack\\python3.9\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (4.4.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in d:\\cs_hack\\python3.9\\lib\\site-packages (from packaging->datasets) (3.0.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\cs_hack\\python3.9\\lib\\site-packages (from requests>=2.19.0->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in d:\\cs_hack\\python3.9\\lib\\site-packages (from requests>=2.19.0->datasets) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\cs_hack\\python3.9\\lib\\site-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
      "Requirement already satisfied: colorama in c:\\users\\betty kha\\appdata\\roaming\\python\\python39\\site-packages (from tqdm>=4.62.1->datasets) (0.4.4)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\cs_hack\\python3.9\\lib\\site-packages (from pandas->datasets) (2022.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in d:\\cs_hack\\python3.9\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\betty kha\\appdata\\roaming\\python\\python39\\site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.15.0)\n",
      "Installing collected packages: xxhash, pyyaml, pyarrow, multidict, fsspec, frozenlist, dill, async-timeout, yarl, responses, multiprocess, huggingface-hub, aiosignal, aiohttp, datasets\n",
      "Successfully installed aiohttp-3.8.3 aiosignal-1.3.1 async-timeout-4.0.2 datasets-2.8.0 dill-0.3.6 frozenlist-1.3.3 fsspec-2022.11.0 huggingface-hub-0.11.1 multidict-6.0.4 multiprocess-0.70.14 pyarrow-10.0.1 pyyaml-6.0 responses-0.18.0 xxhash-3.2.0 yarl-1.8.2\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# train_dataset = load_dataset('food101', split='train')\n",
    "# test_dataset = load_dataset('food101', split='test')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# working\n",
    "\n",
    "dataset_path = '../food101/train/food-101/images'\n",
    "json_path = dataset_path + '/ing_jsn.json'\n",
    "\n",
    "#choose random seed\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# train_dataset = torchvision.datasets.Food101(dataset_path, split='train', download=True, transform=transforms)\n",
    "# test_dataset = torchvision.datasets.Food101(dataset_path, split='test', download=True, transform=transforms)\n",
    "\n",
    "# train_dataset = datasets.ImageFolder(dataset_path +'/train/food-101/', transform=transforms)\n",
    "# test_dataset =datasets.ImageFolder(dataset_path + '/test/food-101/'  ,  transform=transforms)\n",
    "\n",
    "# train_dataset = load_dataset('json', data_files= os.path.join( dataset_path,\"train\",\"food101\", \"meta\", \"train.json\" ) , split='train[:80%]')\n",
    "# test_dataset = load_dataset('json', data_files=dataset_path + '/meta/'+'test.json', split='test[:20%]')\n",
    "transforms = transforms.Compose(\n",
    "[\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((224,224))\n",
    "])\n",
    "\n",
    "dataset = IngredientsDataset(json_path, dataset_path, transforms)\n",
    "\n",
    "# split train and test\n",
    "train_size = int(0.8 * dataset.__len__())\n",
    "test_size = dataset.__len__() - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)  # not working well\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=4, shuffle=True)\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def imshow(inp, title=None):\n",
    "\n",
    "\tinp = inp.cpu() # if device else inp\n",
    "\tinp = inp.numpy().transpose((1, 2, 0))\n",
    "\n",
    "\t# mean = np.array([0.485, 0.456, 0.406])\n",
    "\t# std = np.array([0.229, 0.224, 0.225])\n",
    "\t# inp = std * inp + mean\n",
    "\t# inp = np.clip(inp, 0, 1)\n",
    "\n",
    "\tplt.imshow(inp)\n",
    "\tif title is not None:\n",
    "\t\tplt.title(title)\n",
    "\tplt.pause(0.001)\n",
    "\n",
    "tmp = next(iter(train_dataloader))\n",
    "print(\"image:\", tmp['image'])\n",
    "\n",
    "\n",
    "# out = torchvision.utils.make_grid(images)\n",
    "# print(\"out-size:\", out.shape)\n",
    "\n",
    "imshow(tmp['image'][0,:,:,:], title='try')  # [train_dataset.classes[x] for x in labels])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "No such file: 'D:\\project_deep\\CLIPyourFood\\food101\\train\\food-101\\images\\cheesecake\\3388138'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[7], line 40\u001B[0m\n\u001B[0;32m     37\u001B[0m         plt\u001B[38;5;241m.\u001B[39mtitle(title)\n\u001B[0;32m     38\u001B[0m     plt\u001B[38;5;241m.\u001B[39mpause(\u001B[38;5;241m0.001\u001B[39m)\n\u001B[1;32m---> 40\u001B[0m images, labels \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mnext\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43miter\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mtrain_dataloader\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     41\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mimages-size:\u001B[39m\u001B[38;5;124m\"\u001B[39m, images\u001B[38;5;241m.\u001B[39mshape)\n\u001B[0;32m     42\u001B[0m \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[0;32m     43\u001B[0m \u001B[38;5;66;03m# out = torchvision.utils.make_grid(images)\u001B[39;00m\n\u001B[0;32m     44\u001B[0m \u001B[38;5;66;03m# print(\"out-size:\", out.shape)\u001B[39;00m\n",
      "File \u001B[1;32mD:\\project_deep\\deep_env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:628\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    625\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    626\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[0;32m    627\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[1;32m--> 628\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    629\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m    630\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    631\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    632\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[1;32mD:\\project_deep\\deep_env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:671\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    669\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    670\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m--> 671\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m    672\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[0;32m    673\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[1;32mD:\\project_deep\\deep_env\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:58\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[1;34m(self, possibly_batched_index)\u001B[0m\n\u001B[0;32m     56\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[0;32m     57\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m---> 58\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[idx] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[0;32m     59\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     60\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "File \u001B[1;32mD:\\project_deep\\deep_env\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:58\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m     56\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[0;32m     57\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m---> 58\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[0;32m     59\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     60\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "File \u001B[1;32mD:\\project_deep\\deep_env\\lib\\site-packages\\torch\\utils\\data\\dataset.py:295\u001B[0m, in \u001B[0;36mSubset.__getitem__\u001B[1;34m(self, idx)\u001B[0m\n\u001B[0;32m    293\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(idx, \u001B[38;5;28mlist\u001B[39m):\n\u001B[0;32m    294\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[[\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mindices[i] \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m idx]]\n\u001B[1;32m--> 295\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mindices\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m\u001B[43m]\u001B[49m\n",
      "File \u001B[1;32mD:\\project_deep\\CLIPyourFood\\Data\\IngredientsLoader.py:43\u001B[0m, in \u001B[0;36mIngredientsDataset.__getitem__\u001B[1;34m(self, idx)\u001B[0m\n\u001B[0;32m     41\u001B[0m img_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mimages[idx]\n\u001B[0;32m     42\u001B[0m img_path \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mroot_dir, img_name)\n\u001B[1;32m---> 43\u001B[0m image \u001B[38;5;241m=\u001B[39m \u001B[43mio\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mimread\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg_path\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     44\u001B[0m ingredients \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mingredients_frame[img_name]\n\u001B[0;32m     45\u001B[0m sample \u001B[38;5;241m=\u001B[39m {\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mimage\u001B[39m\u001B[38;5;124m'\u001B[39m: image, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mingredients\u001B[39m\u001B[38;5;124m'\u001B[39m: ingredients}\n",
      "File \u001B[1;32mD:\\project_deep\\deep_env\\lib\\site-packages\\skimage\\io\\_io.py:53\u001B[0m, in \u001B[0;36mimread\u001B[1;34m(fname, as_gray, plugin, **plugin_args)\u001B[0m\n\u001B[0;32m     50\u001B[0m         plugin \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtifffile\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m     52\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m file_or_url_context(fname) \u001B[38;5;28;01mas\u001B[39;00m fname:\n\u001B[1;32m---> 53\u001B[0m     img \u001B[38;5;241m=\u001B[39m call_plugin(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mimread\u001B[39m\u001B[38;5;124m'\u001B[39m, fname, plugin\u001B[38;5;241m=\u001B[39mplugin, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mplugin_args)\n\u001B[0;32m     55\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(img, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mndim\u001B[39m\u001B[38;5;124m'\u001B[39m):\n\u001B[0;32m     56\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m img\n",
      "File \u001B[1;32mD:\\project_deep\\deep_env\\lib\\site-packages\\skimage\\io\\manage_plugins.py:207\u001B[0m, in \u001B[0;36mcall_plugin\u001B[1;34m(kind, *args, **kwargs)\u001B[0m\n\u001B[0;32m    203\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mIndexError\u001B[39;00m:\n\u001B[0;32m    204\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mCould not find the plugin \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m for \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;241m%\u001B[39m\n\u001B[0;32m    205\u001B[0m                            (plugin, kind))\n\u001B[1;32m--> 207\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mD:\\project_deep\\deep_env\\lib\\site-packages\\skimage\\io\\_plugins\\imageio_plugin.py:15\u001B[0m, in \u001B[0;36mimread\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     13\u001B[0m \u001B[38;5;129m@wraps\u001B[39m(imageio_imread)\n\u001B[0;32m     14\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mimread\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m---> 15\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m np\u001B[38;5;241m.\u001B[39masarray(imageio_imread(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs))\n",
      "File \u001B[1;32mD:\\project_deep\\deep_env\\lib\\site-packages\\imageio\\v2.py:226\u001B[0m, in \u001B[0;36mimread\u001B[1;34m(uri, format, **kwargs)\u001B[0m\n\u001B[0;32m    223\u001B[0m imopen_args \u001B[38;5;241m=\u001B[39m decypher_format_arg(\u001B[38;5;28mformat\u001B[39m)\n\u001B[0;32m    224\u001B[0m imopen_args[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlegacy_mode\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m--> 226\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m imopen(uri, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mri\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mimopen_args) \u001B[38;5;28;01mas\u001B[39;00m file:\n\u001B[0;32m    227\u001B[0m     result \u001B[38;5;241m=\u001B[39m file\u001B[38;5;241m.\u001B[39mread(index\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    229\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
      "File \u001B[1;32mD:\\project_deep\\deep_env\\lib\\site-packages\\imageio\\core\\imopen.py:113\u001B[0m, in \u001B[0;36mimopen\u001B[1;34m(uri, io_mode, plugin, extension, format_hint, legacy_mode, **kwargs)\u001B[0m\n\u001B[0;32m    111\u001B[0m     request\u001B[38;5;241m.\u001B[39mformat_hint \u001B[38;5;241m=\u001B[39m format_hint\n\u001B[0;32m    112\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 113\u001B[0m     request \u001B[38;5;241m=\u001B[39m \u001B[43mRequest\u001B[49m\u001B[43m(\u001B[49m\u001B[43muri\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mio_mode\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mformat_hint\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mformat_hint\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mextension\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mextension\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    115\u001B[0m source \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m<bytes>\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(uri, \u001B[38;5;28mbytes\u001B[39m) \u001B[38;5;28;01melse\u001B[39;00m uri\n\u001B[0;32m    117\u001B[0m \u001B[38;5;66;03m# fast-path based on plugin\u001B[39;00m\n\u001B[0;32m    118\u001B[0m \u001B[38;5;66;03m# (except in legacy mode)\u001B[39;00m\n",
      "File \u001B[1;32mD:\\project_deep\\deep_env\\lib\\site-packages\\imageio\\core\\request.py:248\u001B[0m, in \u001B[0;36mRequest.__init__\u001B[1;34m(self, uri, mode, extension, format_hint, **kwargs)\u001B[0m\n\u001B[0;32m    245\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInvalid Request.Mode: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmode\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    247\u001B[0m \u001B[38;5;66;03m# Parse what was given\u001B[39;00m\n\u001B[1;32m--> 248\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_parse_uri\u001B[49m\u001B[43m(\u001B[49m\u001B[43muri\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    250\u001B[0m \u001B[38;5;66;03m# Set extension\u001B[39;00m\n\u001B[0;32m    251\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m extension \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[1;32mD:\\project_deep\\deep_env\\lib\\site-packages\\imageio\\core\\request.py:408\u001B[0m, in \u001B[0;36mRequest._parse_uri\u001B[1;34m(self, uri)\u001B[0m\n\u001B[0;32m    405\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_read_request:\n\u001B[0;32m    406\u001B[0m     \u001B[38;5;66;03m# Reading: check that the file exists (but is allowed a dir)\u001B[39;00m\n\u001B[0;32m    407\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mexists(fn):\n\u001B[1;32m--> 408\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mFileNotFoundError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNo such file: \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m fn)\n\u001B[0;32m    409\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    410\u001B[0m     \u001B[38;5;66;03m# Writing: check that the directory to write to does exist\u001B[39;00m\n\u001B[0;32m    411\u001B[0m     dn \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mdirname(fn)\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: No such file: 'D:\\project_deep\\CLIPyourFood\\food101\\train\\food-101\\images\\cheesecake\\3388138'"
     ]
    }
   ],
   "source": [
    "#preparing the data\n",
    "dataset_path = 'food101/train/food-101/images'\n",
    "json_path = dataset_path + '/ing_jsn.json'\n",
    "\n",
    "# train_dataset = torchvision.datasets.Food101(dataset_path, split='train', download=True, transform=transforms)\n",
    "# test_dataset = torchvision.datasets.Food101(dataset_path, split='test', download=True, transform=transforms)\n",
    "\n",
    "# train_dataset = datasets.ImageFolder(dataset_path +'/train/food-101/', transform=transforms)\n",
    "# test_dataset =datasets.ImageFolder(dataset_path + '/test/food-101/'  ,  transform=transforms)\n",
    "\n",
    "# train_dataset = load_dataset('json', data_files= os.path.join( dataset_path,\"train\",\"food101\", \"meta\", \"train.json\" ) , split='train[:80%]')\n",
    "# test_dataset = load_dataset('json', data_files=dataset_path + '/meta/'+'test.json', split='test[:20%]')\n",
    "\n",
    "dataset = IngredientsLoader.IngredientsDataset(json_path,dataset_path, transforms)\n",
    "\n",
    "#split train and test\n",
    "train_size = int(0.8 * dataset.__len__())\n",
    "test_size =dataset.__len__() - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True) #not working well\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def imshow(inp, title=None):\n",
    "\n",
    "    inp = inp.cpu() if device else inp\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    inp = std * inp + mean\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "\n",
    "    plt.imshow(inp)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.001)\n",
    "\n",
    "images, labels = next(iter(train_dataloader))\n",
    "print(\"images-size:\", images.shape)\n",
    "#\n",
    "# out = torchvision.utils.make_grid(images)\n",
    "# print(\"out-size:\", out.shape)\n",
    "\n",
    "imshow(images[0], title=labels[0])#[train_dataset.classes[x] for x in labels])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\CS_HACK\\python3.9\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "D:\\CS_HACK\\python3.9\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": "ResNet(\n  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (relu): ReLU(inplace=True)\n  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n  (layer1): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer2): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer3): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer4): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n  (fc): Linear(in_features=512, out_features=1000, bias=True)\n)"
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model\n",
    "net = models.resnet18(pretrained=True)\n",
    "#net = net.cuda() if device else net\n",
    "net"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './food101/test/food-101/images/lasagna\\\\2846781.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[1;32mIn [54]\u001B[0m, in \u001B[0;36m<cell line: 18>\u001B[1;34m()\u001B[0m\n\u001B[0;32m     15\u001B[0m     epoch_acc \u001B[38;5;241m=\u001B[39m running_corrects\u001B[38;5;241m.\u001B[39mdouble() \u001B[38;5;241m/\u001B[39m \u001B[38;5;28mlen\u001B[39m(dataloader\u001B[38;5;241m.\u001B[39mdataset)\n\u001B[0;32m     16\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m epoch_loss, epoch_acc\n\u001B[1;32m---> 18\u001B[0m \u001B[43mevaluate_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnet\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_dataloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcriterion\u001B[49m\u001B[43m)\u001B[49m\n",
      "Input \u001B[1;32mIn [54]\u001B[0m, in \u001B[0;36mevaluate_model\u001B[1;34m(model, dataloader, criterion)\u001B[0m\n\u001B[0;32m      4\u001B[0m running_loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.0\u001B[39m\n\u001B[0;32m      5\u001B[0m running_corrects \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m----> 6\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m inputs, labels \u001B[38;5;129;01min\u001B[39;00m dataloader:\n\u001B[0;32m      7\u001B[0m     inputs \u001B[38;5;241m=\u001B[39m inputs\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[0;32m      8\u001B[0m     labels \u001B[38;5;241m=\u001B[39m labels\u001B[38;5;241m.\u001B[39mto(device)\n",
      "File \u001B[1;32mD:\\CS_HACK\\python3.9\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:628\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    625\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    626\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[0;32m    627\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[1;32m--> 628\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    629\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m    630\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    631\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    632\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[1;32mD:\\CS_HACK\\python3.9\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:671\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    669\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    670\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m--> 671\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m    672\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[0;32m    673\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[1;32mD:\\CS_HACK\\python3.9\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:58\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[1;34m(self, possibly_batched_index)\u001B[0m\n\u001B[0;32m     56\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[0;32m     57\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m---> 58\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[idx] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[0;32m     59\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     60\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "File \u001B[1;32mD:\\CS_HACK\\python3.9\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:58\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m     56\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[0;32m     57\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m---> 58\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[0;32m     59\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     60\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "File \u001B[1;32mD:\\CS_HACK\\python3.9\\lib\\site-packages\\torchvision\\datasets\\folder.py:229\u001B[0m, in \u001B[0;36mDatasetFolder.__getitem__\u001B[1;34m(self, index)\u001B[0m\n\u001B[0;32m    221\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    222\u001B[0m \u001B[38;5;124;03mArgs:\u001B[39;00m\n\u001B[0;32m    223\u001B[0m \u001B[38;5;124;03m    index (int): Index\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    226\u001B[0m \u001B[38;5;124;03m    tuple: (sample, target) where target is class_index of the target class.\u001B[39;00m\n\u001B[0;32m    227\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    228\u001B[0m path, target \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msamples[index]\n\u001B[1;32m--> 229\u001B[0m sample \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mloader\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    230\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransform \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    231\u001B[0m     sample \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransform(sample)\n",
      "File \u001B[1;32mD:\\CS_HACK\\python3.9\\lib\\site-packages\\torchvision\\datasets\\folder.py:268\u001B[0m, in \u001B[0;36mdefault_loader\u001B[1;34m(path)\u001B[0m\n\u001B[0;32m    266\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m accimage_loader(path)\n\u001B[0;32m    267\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 268\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mpil_loader\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\CS_HACK\\python3.9\\lib\\site-packages\\torchvision\\datasets\\folder.py:246\u001B[0m, in \u001B[0;36mpil_loader\u001B[1;34m(path)\u001B[0m\n\u001B[0;32m    244\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mpil_loader\u001B[39m(path: \u001B[38;5;28mstr\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Image\u001B[38;5;241m.\u001B[39mImage:\n\u001B[0;32m    245\u001B[0m     \u001B[38;5;66;03m# open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\u001B[39;00m\n\u001B[1;32m--> 246\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mrb\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m f:\n\u001B[0;32m    247\u001B[0m         img \u001B[38;5;241m=\u001B[39m Image\u001B[38;5;241m.\u001B[39mopen(f)\n\u001B[0;32m    248\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m img\u001B[38;5;241m.\u001B[39mconvert(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRGB\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: './food101/test/food-101/images/lasagna\\\\2846781.jpg'"
     ]
    }
   ],
   "source": [
    "#evaluation of model on test set\n",
    "def evaluate_model(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    for inputs, labels in dataloader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        loss = criterion(outputs, labels)\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "    epoch_loss = running_loss / len(dataloader.dataset)\n",
    "    epoch_acc = running_corrects.double() / len(dataloader.dataset)\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "evaluate_model(net, test_dataloader, criterion)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=momentum)\n",
    "def accuracy(out, labels):\n",
    "    _,pred = torch.max(out, dim=1)\n",
    "    return torch.sum(pred==labels).item()\n",
    "\n",
    "num_ftrs = net.fc.in_features\n",
    "net.fc = nn.Linear(num_ftrs, 128)\n",
    "net.fc = net.fc.cuda() if use_cuda else net.fc"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "n_epochs = 5\n",
    "print_every = 10\n",
    "valid_loss_min = np.Inf\n",
    "val_loss = []\n",
    "val_acc = []\n",
    "train_loss = []\n",
    "train_acc = []\n",
    "total_step = len(train_dataloader)\n",
    "for epoch in range(1, n_epochs+1):\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total=0\n",
    "    print(f'Epoch {epoch}\\n')\n",
    "    for batch_idx, (data_, target_) in enumerate(train_dataloader):\n",
    "        data_, target_ = data_.to(device), target_.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = net(data_)\n",
    "        loss = criterion(outputs, target_)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _,pred = torch.max(outputs, dim=1)\n",
    "        correct += torch.sum(pred==target_).item()\n",
    "        total += target_.size(0)\n",
    "        if (batch_idx) % 20 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
    "                   .format(epoch, n_epochs, batch_idx, total_step, loss.item()))\n",
    "    train_acc.append(100 * correct / total)\n",
    "    train_loss.append(running_loss/total_step)\n",
    "    print(f'\\ntrain-loss: {np.mean(train_loss):.4f}, train-acc: {(100 * correct/total):.4f}')\n",
    "    batch_loss = 0\n",
    "    total_t=0\n",
    "    correct_t=0\n",
    "    with torch.no_grad():\n",
    "        net.eval()\n",
    "        for data_t, target_t in (test_dataloader):\n",
    "            data_t, target_t = data_t.to(device), target_t.to(device)\n",
    "            outputs_t = net(data_t)\n",
    "            loss_t = criterion(outputs_t, target_t)\n",
    "            batch_loss += loss_t.item()\n",
    "            _,pred_t = torch.max(outputs_t, dim=1)\n",
    "            correct_t += torch.sum(pred_t==target_t).item()\n",
    "            total_t += target_t.size(0)\n",
    "        val_acc.append(100 * correct_t/total_t)\n",
    "        val_loss.append(batch_loss/len(test_dataloader))\n",
    "        network_learned = batch_loss < valid_loss_min\n",
    "        print(f'validation loss: {np.mean(val_loss):.4f}, validation acc: {(100 * correct_t/total_t):.4f}\\n')\n",
    "\n",
    "\n",
    "        if network_learned:\n",
    "            valid_loss_min = batch_loss\n",
    "            torch.save(net.state_dict(), 'resnet.pt')\n",
    "            print('Improvement-Detected, save-model')\n",
    "    net.train()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#plot statistics\n",
    "fig = plt.figure(figsize=(20,10))\n",
    "plt.title(\"Train-Validation Accuracy\")\n",
    "plt.plot(train_acc, label='train')\n",
    "plt.plot(val_acc, label='validation')\n",
    "plt.xlabel('num_epochs', fontsize=12)\n",
    "plt.ylabel('accuracy', fontsize=12)\n",
    "plt.legend(loc='best')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#visualize the model\n",
    "def visualize_model(net, num_images=4):\n",
    "    images_so_far = 0\n",
    "    fig = plt.figure(figsize=(15, 10))\n",
    "\n",
    "    for i, data in enumerate(test_dataloader):\n",
    "        inputs, labels = data\n",
    "        if use_cuda:\n",
    "            inputs, labels = inputs.cuda(), labels.cuda()\n",
    "        outputs = net(inputs)\n",
    "        _, preds = torch.max(outputs.data, 1)\n",
    "        preds = preds.cpu().numpy() if use_cuda else preds.numpy()\n",
    "        for j in range(inputs.size()[0]):\n",
    "            images_so_far += 1\n",
    "            ax = plt.subplot(2, num_images//2, images_so_far)\n",
    "            ax.axis('off')\n",
    "            ax.set_title('predictes: {}'.format(test_dataset.classes[preds[j]]))\n",
    "            imshow(inputs[j])\n",
    "\n",
    "            if images_so_far == num_images:\n",
    "                return\n",
    "\n",
    "plt.ion()\n",
    "visualize_model(net)\n",
    "plt.ioff()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}